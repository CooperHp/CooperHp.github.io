<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;example.com&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;}}</script>
<meta name="description" content="历史发展 上世纪90年代~本世纪前10年，由于SVM等机器学习算法的提出导致人工神经网络的发展陷入低潮期。多层神经网络的劣势在于：  数学不漂亮，优化算法只能获取局部极值，算法性能与初始值有关 不可解释性。训练神经网络获得的参数与实际任务的关联性非常模糊 模型可调整的参数（如网络层数，每层神经元，非线性函数，学习率，优化方法，终止条件...）个数很多，使得训练神经网络编程了一门“艺术”（SVM只需">
<meta property="og:type" content="article">
<meta property="og:title" content="4. 深度学习">
<meta property="og:url" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hu说扒道">
<meta property="og:description" content="历史发展 上世纪90年代~本世纪前10年，由于SVM等机器学习算法的提出导致人工神经网络的发展陷入低潮期。多层神经网络的劣势在于：  数学不漂亮，优化算法只能获取局部极值，算法性能与初始值有关 不可解释性。训练神经网络获得的参数与实际任务的关联性非常模糊 模型可调整的参数（如网络层数，每层神经元，非线性函数，学习率，优化方法，终止条件...）个数很多，使得训练神经网络编程了一门“艺术”（SVM只需">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图0.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图1.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图2.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图3.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图4.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图5.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图6.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图7.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图8.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图9.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图10.png">
<meta property="og:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图11.png">
<meta property="article:published_time" content="2021-08-22T13:09:48.000Z">
<meta property="article:modified_time" content="2021-08-22T13:09:48.000Z">
<meta property="article:author" content="Cooper">
<meta property="article:tag" content="数学物理">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图0.png">


<link rel="canonical" href="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-Hans&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;http:&#x2F;&#x2F;example.com&#x2F;%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89&#x2F;%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&#x2F;&quot;,&quot;path&quot;:&quot;学习笔记&#x2F;《机器学习》（浙大公开课）&#x2F;《机器学习》-4-深度学习&#x2F;&quot;,&quot;title&quot;:&quot;4. 深度学习&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>4. 深度学习 | Hu说扒道</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hu说扒道</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">做自己喜欢的事。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%86%E5%8F%B2%E5%8F%91%E5%B1%95"><span class="nav-number">1.</span> <span class="nav-text">历史发展</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">2.</span> <span class="nav-text">自编码器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E6%AF%8F%E5%B1%82%E8%AE%AD%E7%BB%83"><span class="nav-number">2.1.</span> <span class="nav-text">自编码器每层训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet"><span class="nav-number">3.</span> <span class="nav-text">卷积神经网络LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">3.1.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%8D%E9%87%87%E6%A0%B7%E5%B1%82"><span class="nav-number">3.2.</span> <span class="nav-text">降采样层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lenet%E7%BD%91%E7%BB%9C"><span class="nav-number">3.3.</span> <span class="nav-text">LeNet网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Calexnet"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络AlexNet</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6"><span class="nav-number">5.</span> <span class="nav-text">深度学习编程框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%91%E5%B9%B4%E6%9D%A5%E6%B5%81%E8%A1%8C%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6"><span class="nav-number">6.</span> <span class="nav-text">近年来流行的卷积神经网络框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#vggnet"><span class="nav-number">6.1.</span> <span class="nav-text">VGGNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#googlenet"><span class="nav-number">6.2.</span> <span class="nav-text">GoogleNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#resnet"><span class="nav-number">6.3.</span> <span class="nav-text">ResNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">7.</span> <span class="nav-text">损失函数的改进</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E5%89%B2"><span class="nav-number">8.</span> <span class="nav-text">目标检测与分割</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-number">8.1.</span> <span class="nav-text">多目标检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="nav-number">8.2.</span> <span class="nav-text">语义分割</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8Brnn%E5%92%8Clstm"><span class="nav-number">9.</span> <span class="nav-text">时间序列的深度学习模型（RNN和LSTM）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn"><span class="nav-number">9.1.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lstm"><span class="nav-number">9.2.</span> <span class="nav-text">LSTM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="nav-number">10.</span> <span class="nav-text">生成对抗网络</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Cooper"
      src="/images/naruto.jpg">
  <p class="site-author-name" itemprop="name">Cooper</p>
  <div class="site-description" itemprop="description">分享一些个人的学习笔记、资料和想法。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="/images/wechat.jpg" title="微信 → &#x2F;images&#x2F;wechat.jpg"><i class="fab fa-weixin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zai-xia-shu-liao-73" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;zai-xia-shu-liao-73" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/naruto.jpg">
      <meta itemprop="name" content="Cooper">
      <meta itemprop="description" content="分享一些个人的学习笔记、资料和想法。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hu说扒道">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          4. 深度学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-08-22 21:09:48" itemprop="dateCreated datePublished" datetime="2021-08-22T21:09:48+08:00">2021-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">《机器学习》（浙大公开课）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="历史发展">历史发展</h1>
<p>上世纪90年代~本世纪前10年，由于SVM等机器学习算法的提出导致人工神经网络的发展陷入低潮期。多层神经网络的劣势在于：</p>
<ul>
<li>数学不漂亮，优化算法只能获取局部极值，算法性能与初始值有关</li>
<li>不可解释性。训练神经网络获得的参数与实际任务的关联性非常模糊</li>
<li>模型可调整的参数（如网络层数，每层神经元，非线性函数，学习率，优化方法，终止条件...）个数很多，使得训练神经网络编程了一门“艺术”（SVM只需要少量参数）</li>
<li>如果要训练相对复杂的网络，需要大量的训练样本（SVM只需要小规模数据集） <span id="more"></span></li>
</ul>
<p>Hinton认为人工神经网络算法的潜力没有挖掘出来，他认为构建深度的神经网络系统才有可能人工神经网络算法发挥其潜力。“深度”指的是神经网络中的层数更多，每层的神经元个数更多。</p>
<p>传统的后向传播算法待估计参数的初始化采用随机化权重系数和偏置项的方式进行初始化，在参数个数很多的时候，此方法会较大概率收敛到一个表现比较差的局部极值。如果待估计参数一开始就在一个更优的局部极值附近，就有可能克服上述问题，获得更好的神经网络模型。</p>
<p>2006年，Hinton提出自编码器（auto-encoder）作为深度神经网络参数初始化方法，以此对数据进行降维处理，克服了传统方法的过学习问题。神经网络再次回到热潮期，深度学习引发革新。</p>
<p>深度学习引发的革新：</p>
<ul>
<li>2009年，微软公司Li Deng和Dong Yu等人将深度神经网络和自编码器引入语音识别中，将大词汇连续语音识别系统的识别率提高了10个百分点以上。使语音识别落地生根</li>
<li>数据库的构建。Fei-fei Li等人从2007年开始创造了一个大规模图像识别数据库ImageNet，为深度学习在图像处理领域的发展奠定了数据的基础</li>
<li>2013年，Hinton的学生Alex Krizhevsky侯建一个包含65万多个神经元，待估计参数超过6000万的大规模的神经网络。采用ImageNet进行训练的模型获得第一，击败Google和Facebook的团队</li>
<li>2016年3月，英国DeepMind公司基于深度学习和强化学习的围棋程序AlphaGo以4:1击败李世石</li>
</ul>
<p>其他深度学习架构模型：</p>
<ul>
<li>生成对抗网络：Generative Adversarial Networks，GAN</li>
<li>循环神经网络：Recurrent Neural Networks，RNN</li>
<li>图卷积神经网络：Graph Convolutional Neural Networks，GCNN</li>
<li>...</li>
</ul>
<h1 id="自编码器">自编码器</h1>
<p>自编码器采用的是分层初始化的思想，每一层都不同程度的编码了训练样本的信息，因此能够大概率获得较佳的局部极小值。其大致思想如下：</p>
<p>对于如下深度网络模型：</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图0.png" width="500"></p>
<ul>
<li><p>步骤一：训练如下网络 <img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图1.png" width="500"></p></li>
<li><p>步骤二：训练好第一层后，接着训练第二层。在训练完第一层后，将输入层及第一层的参数原封不动的拿到步骤二采用BP算法计算第二层参数。</p></li>
</ul>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图2.png" width="500"></p>
<ul>
<li>步骤M：以此类推，训练好M-1层后，接着训练第M层</li>
</ul>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图3.png" width="500"></p>
<ul>
<li>当完成所有N层的自编码器训练后，将自编码器训练的参数作为初始值，再用BP算法调整整个网络的参数</li>
</ul>
<h2 id="自编码器每层训练">自编码器每层训练</h2>
<p>对于一个深度网络，假设<span class="math inline">\(X\)</span>是四维，其第一层有3个神经元，那么自编码器如下图所示。自编码器的输入和输出都是原网络的<span class="math inline">\(X\)</span>，中间只有一层神经元是原网络的第一层。四维数据首先被压缩为三维，该三维数据能够大致恢复原有的四维数据<span class="math inline">\(X\)</span>。该网络使得中间层的三个神经元浓缩了原有的四维数据的信息，可看作原有四维数据的编码。由于是<span class="math inline">\(X\)</span>到期自身的编码，因此称为自编码器。可使用后向传播计算该网络参数。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图4.png" width="500"></p>
<p>近年来，自编码器作为一种特征提取器应用非常广泛。如GAN部分借鉴自编码器思想。</p>
<h1 id="卷积神经网络lenet">卷积神经网络LeNet</h1>
<p>上世纪90年，LeCun提出目前最常用的卷积神经网络（Convolutional Neural Networks，CNN），他用自己的名字命名LeNet网络。创建MNIST数据集，用于识别手写体数字0~9的训练。LeCun在该网络中定义了目前CNN中常用的技术，包括自动学习卷积核，多层结构，降采样和全连接技术等。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图5.png" width="500"></p>
<p>LeNet网络由输入层、两个卷积层C1和C3，两个降采样层S2和S4，两个全连接层C5和F6，输出层组成。</p>
<h2 id="卷积层">卷积层</h2>
<p>卷积运算的全过程可以表示如下：假设输入为<span class="math inline">\(32×32×3\)</span>的彩色图片，可以表示为一个<span class="math inline">\(32×32×3\)</span>张量（3是通道数），将图像与卷积核（convolution kernel）进行卷积运算，假设卷积核是<span class="math inline">\(5×5×3\)</span>的张量。该卷积核从左上到右下以步长<span class="math inline">\(stride=1\)</span>进行滑动，每滑到一个位置，把卷积核与该位置图像重合的像素值相乘后相加得到一个数，将每次滑动的数排列起来得到一个<span class="math inline">\(28×28×1\)</span>的张量，称该张量为特征图（feature map）。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图6.png" width="500"></p>
<p>图像张量大小为：<span class="math inline">\(32×32×3\)</span>；卷积核张量大小为：<span class="math inline">\(5×5×3\)</span>；特征图张量大小：<span class="math inline">\((32-5+1)×(32-5+1)×(3-3+1)\)</span>。</p>
<p>在LeNet中，第一层卷积层C1用6个卷积核对原图像进行卷积，获得6个<span class="math inline">\(28×28×6\)</span>的特征图。6个卷积核的参数通过后向传播算法进行更新，可采用链式求导法则计算梯度。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图7.png" width="500"></p>
<h2 id="降采样层">降采样层</h2>
<p>以C1层得到6个<span class="math inline">\(28×28\)</span>的特征图，把这些特征图通过降采样层获得6个<span class="math inline">\(14×14\)</span>的特征图。在LeNet中，降采样层采用取平均值的方式，即将相邻4个值取平均，称为平均降采样（average subsampling）。以步长(2,2)对原特征图进行降采样，将相邻4个值取平均，将<span class="math inline">\(28×28\)</span>的特征图变为<span class="math inline">\(14×14\)</span>的特征图。降采样层的梯度计算仍采用后向传播算法。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图8.png" width="500"></p>
<h2 id="lenet网络">LeNet网络</h2>
<p>输入层（32×32） → C1卷积层（6个卷积核：5×5；特征图：28×28×6）→ S2降采样层（ 步长(2,2)；特征图14×14）→ C3卷积层（16个卷积核：5×5×6；特征图：10×10×16）→ S4降采样层（步长(2,2)；特征图5×5×16）→ C5全连接层（以16个5×5展开为400像素作为输入，120个神经元，输出120维向量）→ F6全连接层（84个神经元，输出84维向量）→ 输出层（全连接层，Softmax + Cross Entropy的目标函数，10维）</p>
<p>LeNet中待估计参数共计61684个：</p>
<ul>
<li><p>第1层（C1)：<span class="math inline">\(5×5×6=150\)</span></p></li>
<li><p>第2层（S2）：0</p></li>
<li><p>第3层（C3）：<span class="math inline">\(5×5×6×16=2400\)</span></p></li>
<li><p>第4层（S4）：0</p></li>
<li><p>第5层（C5）：<span class="math inline">\((5×5×16+1)×120=48120\)</span></p></li>
<li><p>第6层（F6）：<span class="math inline">\((120+1)×84=10164\)</span></p></li>
<li><p>第7层（输出层）：<span class="math inline">\((84+1)×10=850\)</span></p></li>
</ul>
<blockquote>
<p>Tips：</p>
<ul>
<li>在LeNet中，层与层之间都有非线性函数（激活函数）连接</li>
<li>全连接层中有偏置<span class="math inline">\(b\)</span>，因此要"<span class="math inline">\(+1\)</span>"</li>
</ul>
</blockquote>
<h1 id="卷积神经网络alexnet">卷积神经网络AlexNet</h1>
<p>Hinton的学生Alex Krizhevsky构建了一个包含65万多个神经元，待估计参数超过6000万的大规模卷积神经网络，命名为AlexNet，用于解决ImageNet数据集1000类的分类问题。相比于LeNet，AlexNet具有更深的神经网络层数和每层神经元个数，但基本思想与LeNet一致。AlexNet也是由卷积层、降采样层和全连接层组成，层与层之间也有非线性函数和batch normalization。</p>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图9.png" width="500"></p>
<p>相比于LeNet，AlexNet的改进包括：</p>
<ul>
<li>以ReLU函数（rectified linear units）代替sigmoid或tanh函数作为激活函数。<span class="math inline">\(ReLu=\max (0,x)\)</span>。ReLu函数保证每一次网络中只有很少一部分神经元的参数被更新，可以促进神经网络的收敛</li>
<li>降采样层采用最大池化（maxpooling）代替LeNet中的平均降采样（平均池化），降采样被命名为池化（pooling），将邻近的像素作为一个“池子”重新考虑。maxpooling为最大池化，对每个邻近的像素组成的池子选取最大值作为输出。相比平均池化，最大池化可以使梯度直接传导到最大值，每次更新的参数会进一步减少，从而促进网络收敛</li>
<li>随机丢弃（dropout）：为了避免系统参数更新过快导致过拟合，每次利用训练样本更新参数时，随机“丢弃”一定比例的神经元，被丢弃的神经元将不参加训练过程，输入和输出该神经元的权重系数也不做更新。每次训练时的网络结构都不一样，但都分享共同的权重系数。随机丢弃的做法是对每一层，每次训练时以概率P丢弃一些神经元。dropout减缓了网络收敛的速度，以大概率避免了过拟合的发生</li>
<li>数据扩增（data augumentation），即增加训练样本。采用多种方法增加训练样本：
<ul>
<li>将原图水平翻转，将256×256的图像随机选取224×224的片段作为输入图像，可以将一幅图变为2048幅图像</li>
<li>对每幅图引入一定噪声增加样本</li>
</ul></li>
<li>利用GPU（graphic processing unit，显卡）加速训练，GPU特点是线性计算并行化程度好，用于神经网络后向传播算法的梯度更新</li>
</ul>
<h1 id="深度学习编程框架">深度学习编程框架</h1>
<ul>
<li><code>pytorch</code></li>
<li><code>tensorflow</code></li>
<li><code>caffe</code></li>
</ul>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图10.png" width="500"></p>
<h1 id="近年来流行的卷积神经网络框架">近年来流行的卷积神经网络框架</h1>
<h2 id="vggnet">VGGNet</h2>
<p>相比于AlexNet，VGGNet：</p>
<ul>
<li>增加了网络的深度</li>
<li>用多个<span class="math inline">\(3×3\)</span>卷积核叠加代替更大的卷积核，用以增加感受野（receptive field）</li>
</ul>
<p>感受野是卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小，特征图上的一个点对应输入图上的区域。</p>
<p>VGGNet是一个计算和存储开销都很大的网络。</p>
<h2 id="googlenet">GoogleNet</h2>
<p>提出inception结构，用多个小卷积核用固定的方式组合在一起，来代替大的卷积核，以达到增加感受野和减少参数的目的。</p>
<h2 id="resnet">ResNet</h2>
<p>2015年，Kaiming He等人发明ResNet，使训练深层的卷积神经网络成为可能。作者发现训练一个浅层的网络无论是在训练集还是测试集上都比生成网络表现的好。ResNet的思想是将浅层的输出通过线性变换后直接加到深层的输入中。</p>
<p>用紧凑的，小而深的网络代替大而浅的网络，在具体应用中加入一些技巧。例如，ShuffleNet，MobileNet。</p>
<p>网络结构搜索（network architecture search），即如何从一大堆网络结构中搜索适合具体任务的网络结构。</p>
<h1 id="损失函数的改进">损失函数的改进</h1>
<ul>
<li><p>Softmax损失公式： <span class="math display">\[
L_1=\frac {1} {N} \sum _i\log (\frac {e^{f_{y_i}}}{\sum _j e^{f_j}})=-\log(\frac {e^{||\omega_{y_i}||||{x_i}||\cos(\theta_{y_i})}}{\sum _ie^{||\omega_{j}||||{x_i}||\cos(\theta_{y_j})}})
\]</span></p></li>
<li><p>L-softmax （Large-Margin softmax loss）损失公式： <span class="math display">\[
L_2=-\log(\frac {e^{||\omega_{y_i}||||{x_i}||\psi({\theta_{y_i}})}}{e^{||\omega_{y_i}||||{x_i}||\psi({\theta_{y_i}})}+\sum _{j\ne y_i}e^{||\omega_{j}||||{x_i}||\cos(\theta_{j})}})
\]</span></p></li>
<li><p>Cosface损失公式： <span class="math display">\[
L_3=\frac {1}{N}\sum _i-\log (\frac {e^{x(\cos(\theta _{y_i})-m)}}{e^{x(\cos(\theta _{y_i})-m}+\sum _{j=1,j\ne y_i}^ne^{scos(\theta_j)}})
\]</span></p></li>
<li><p>Arcface损失公式： <span class="math display">\[
L_4=\frac {1}{N}\sum _i-\log (\frac {e^{x(\cos(\theta _{y_i}+m))}}{e^{x(\cos(\theta _{y_i}+m))}+\sum _{j=1,j\ne y_i}^ne^{scos(\theta_j)}})
\]</span></p></li>
<li><p>Triplet损失函数（三元组损失函数）： <span class="math display">\[
L_5=\sum_i^N[||f(x_i^a)-f(x_i^p)||-||f(x_i^a)-f(x_i^n)||+\alpha]_+
\]</span></p></li>
</ul>
<h1 id="目标检测与分割">目标检测与分割</h1>
<p><img src="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/图11.png" width="500"></p>
<p>图像分类识别，实际中包括目标定位于识别、目标检测和语义分割及其结合，其实现难度依次增加。</p>
<p>目标定位与识别：在CNN最后一层之后增加四个输出，分别表示左上点的坐标（x，y）、长宽（w，h）</p>
<h2 id="多目标检测">多目标检测</h2>
<p>R-CNN（region with CNN features，2014年）引入提出ROI候选区域（region of proposals or proposals），其主要思想是用selective search产生候选的方框（proposal），将候选方框输入至CNN，用SVM判断检测结果是否正确。</p>
<p>提取图像候选区域的算法（selective search，SS），用efficient Graph-Base Image Segmentation算法将图像进行过分割（over-segmentation）；过分割后的每个region非常小，对每个region进行相似度的判断并融合形成不同尺度的region，每个region对应一个bounding box。RCNN算法中多个候选区域都要单独输入CNN，导致计算量很大，速度太慢。</p>
<p>2015年提出Fast R-CNN，利用ROI-Pooling加速CNN特征提取过程。其思想为用CNN对整幅图进行卷积，在中间某一层的特征图上再用ROI-Pooling归一化每个候选区域的输出。只需要经过一轮卷积操作就能处理整幅图的所有候选区域，从而大大减少计算量。但仍然需要耗费较多时间产生候选区域。</p>
<p>2015年提出Faster R-CNN，利用深度学习自动产生候选区域，从而解决了多目标检测中的计算复杂度问题，完全具备实时处理图像的性能。</p>
<p>Yolo（you only look once）网络。其思想为：将输入图像分成<span class="math inline">\(S×S\)</span>个格子，若某个物体ground truth的中心位置的坐标落入某个格子中，那么该格子负责检测该物体。</p>
<h2 id="语义分割">语义分割</h2>
<p>语义分割不仅需要检测和识别出图像的目标，还要确定每个目标所对应的像素。2015年提出全卷积网络（fully CNN）用于语义分割。</p>
<h1 id="时间序列的深度学习模型rnn和lstm">时间序列的深度学习模型（RNN和LSTM）</h1>
<p>语音识别、行为识别等任务称为针对时间序列（time series）的任务，解决该问题的深度学习网络模型由RNN（recurrent neural network）循环神经网络和LSTM（long-short term memory）。</p>
<h2 id="rnn">RNN</h2>
<ul>
<li>多个输入、多个输出：语音识别、机器翻译</li>
<li>多个输入、一个输出：行为和识别、单词量有限的语音识别</li>
<li>一个输入、多个输出：文本生成（学习并写古诗）、图像注释（根据图像生成描述）</li>
</ul>
<p>RNN的不足：状态之间的转移函数，以及状态到输出的转移函数都过于简单。网络的表现力不够，难以学习到复杂的输入输出关系。</p>
<h2 id="lstm">LSTM</h2>
<p>用多层神经网络模拟RNN中的转移函数，但这样会导致训练的参数过多难以收敛。基于模型的表现力和复杂性，产生了基于LSTM的RNN。</p>
<h1 id="生成对抗网络">生成对抗网络</h1>
<p>Ian Goodfellow在2014年提出生成对抗网络（GAN，generative adversary network），使得计算机程序有了创造力。GAN基本原理为：借助博弈论的思想构造两个深度神经网络，一个称为生成器（generator），另一个称为判别器（discriminator）。让二者相互对抗，在对抗中进步。</p>
<p>GAN的具体应用：</p>
<ul>
<li>真实的人脸生成：输入随机噪声，利用生成对抗网络来生成真实的人脸图片</li>
<li>侧脸转正：给定不同旋转角度下的人脸图片，通过GAN重建正面角度下的人脸图片，可用于侧脸角度的人脸识别等现实应用</li>
<li>图像翻译：从一张（源域）图像到另一张（目标域）图像的转换。可以类比机器翻译，一种语言转换为另一种语言。翻译过程中会保持源域图像的内容不变，但是风格或者一些其他属性变成目标域。主要包括：基于成对训练数据的图像翻译；基于不成对训练数据的图像翻译（CycleGan网络）。</li>
</ul>
<p>GAN的缺点：</p>
<ul>
<li>训练不稳定，难以直观观测训练过程，难以 有效收敛，很多时候收敛需要靠运气</li>
<li>模式崩溃（mode collapse）</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/" rel="tag"># 数学物理</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88%E6%B5%99%E5%A4%A7%E5%85%AC%E5%BC%80%E8%AF%BE%EF%BC%89/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-3-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="3. 人工神经网络">
                  <i class="fa fa-chevron-left"></i> 3. 人工神经网络
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/%E7%A7%91%E7%A0%94%E7%9B%B8%E5%85%B3/Kinetic-Monte-Carlo/1-KMC%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%85%B6%E8%BD%AF%E4%BB%B6%E4%BB%8B%E7%BB%8D/" rel="next" title="1. KMC模拟及其软件介绍">
                  1. KMC模拟及其软件介绍 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cooper</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
